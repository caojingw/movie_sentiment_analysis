---
title: "Ryerson Capstone"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries
```{r}
library(tidyverse)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(tm)
library(klaR)
library(randomForest)
library(pROC)
library(e1071)
library(caret)
```

## loading datasets

```{r}
test<-read_tsv("C:/Users/james/OneDrive/Ryerson/Capstone/all/test.tsv")
train<-read_tsv("C:/Users/james/OneDrive/Ryerson/Capstone/all/train.tsv")
```



## view and exam both datasets

```{r}
glimpse(train)
glimpse(test)
```

## Missing values
```{r}
apply(train, 2, function(x) sum(is.na(x)))
apply(test, 2, function(x) sum(is.na(x)))
which(is.na(train$Phrase))
train[2006,]
train[which(train$SentenceId==76),]
test[which(is.na(test$Phrase)),]

test[which(test$SentenceId==8588),]
```

We examed these two datasets and found 1 missing value in both datasets. It is not recommended to remove it as it would lose the prediction power.



## Average # of phrase and word count of a sentence
```{r}
train %>%
  mutate(length = str_length(Phrase),
         ncap = str_count(Phrase, "[A-Z]"),
         ncap_len = ncap / length,
         nexcl = str_count(Phrase, fixed("!")),
         nquest = str_count(Phrase, fixed("?")),
         npunct = str_count(Phrase, "[[:punct:]]"),
         nword = str_count(Phrase, "\\w+"),
         nsymb = str_count(Phrase, "&|@|#|\\$|%|\\*|\\^"),
         nsmile = str_count(Phrase, "((?::|;|=)(?:-)?(?:\\)|D|P))")) 
 
train %>%
  group_by(SentenceId) %>%
  summarise(n=n()) %>%
  summarise(mean=mean(n))


test %>%
  group_by(SentenceId) %>%
  summarise(n=n()) %>%
  summarise(mean=mean(n))
```


## Sentiment Distribution
```{r}
table(train$Sentiment)/sum(table(train$Sentiment)) * 100
```

## remove stopwords: stopwords are words carrying less sentimental meaning.
```{r}
combined_removed_stopwords<-tokened_combined %>%
  anti_join(stop_words,by=c('word'='word'))
```

## Top 10 common words in train and test datasets
```{r}
topten_commonwords = function(train)
{
    train %>%
    unnest_tokens(word,Phrase) %>%
    filter(!word %in% c(stop_words$word,"movie","film"))%>%
    count(word,sort=TRUE)%>%
    head(30) %>%
    with(wordcloud(word,n ,max.words = 30))}

topten_commonwords(train)
topten_commonwords(test)
```

# train data
```{r}
train_corpus<-Corpus(VectorSource(train$Phrase))

clean_corpus <- function(x){
  x <- tm_map(x, stripWhitespace)
  x <- tm_map(x, removePunctuation)
  x <- tm_map(x, content_transformer(tolower))
  x <- tm_map(x, removeWords, stopwords("en"))
  return(x)
}

train_dtm<-DocumentTermMatrix(train_corpus)
#freq_term<-findFreqTerms(train_dtm,lowfreq = 5)
train_remove<-removeSparseTerms(train_dtm,sparse = 0.995)
train_m<-as.data.frame(as.matrix(train_remove))
```

# splitting the data
```{r}
index_train<-sample(1:nrow(train_m),nrow(train_m)*0.80)
train_new<-train_m[index_train,]
validation_new<-train_m[-index_train,]
```

# training data
```{r}
model_Naive<-NaiveBayes(x=train_new,train$Sentiment[index_train])
pred<-predict(model_Naive,validation_new)

confusionmatix<-table(pred$class,train$Sentiment[-index_train])
confusionMatrix(confusionmatix)

model_rf<-randomForest(x=train_new,y=train$Sentiment[index_train])
pred_rf<-predict(model_rf,validation_new)


confusionmatix_rf<-table(pred_rf,train$Sentiment[-index_train])
confusionMatrix(confusionmatix_rf)
```

# classify the test data using train model
```{r}
test_corpus<-Corpus(VectorSource(test$Phrase))

test_dtm<-DocumentTermMatrix(test_corpus)
test_remove<-removeSparseTerms(test_dtm,sparse = 0.995)
test_m<-as.data.frame(as.matrix(test_remove))
test_m<-test_m[,names(test_m) %in% names(train_new)]

pre_test<-predict(model_Naive,test_m)
```

